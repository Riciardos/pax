#!/usr/bin/env python
""" Processor for Analyzing XENON - command line interface """

import argparse
import os
import sys
import multiprocessing
from itertools import zip_longest

from pax import core

def grouper(iterable, n, fillvalue=None):
    "Collect data into fixed-length chunks or blocks"
    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx"
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)

class Consumer(multiprocessing.Process):

    def __init__(self, task_queue, result_queue,
                 config_names, config_path, config_dict):
        multiprocessing.Process.__init__(self)
        self.task_queue = task_queue
        self.result_queue = result_queue


parser = argparse.ArgumentParser(description="Process XENON data")
parser.add_argument('--input',  default=None, help="File, database or directory to read events from", nargs='?')
parser.add_argument('--output', default=None, help="File, database or directory to write events to",  nargs='?')

# Parallelization control
parser.add_argument('--cpus', default=1, help="Number of CPUs to use. Default is 1; can be 'all'.",  nargs='?')

# Log level control
parser.add_argument('--log', default=None, help="Set log level, e.g. 'debug'")

# Configuration control
# Pass in a name to use a pre-cooked config from config:
parser.add_argument(
    '--config',
    default='XENON100',
    choices=core.get_named_configuration_options(),
    nargs='+',
    help="Name(s) of the pax configuration(s) to use."
)
# ... or pass in a path to your own config file:
parser.add_argument(
    '--config_path',
    default=[],
    nargs='+',
    help="Path(s) of the configuration file(s) to use."
)
# ... or load the configuration from a previous processed data file
parser.add_argument(
    '--redo',
    default='',
    help="Path of the HDF5 data file to redo."
)

# Plotting override
plotting_control_group = parser.add_mutually_exclusive_group()
plotting_control_group.add_argument(
    '--plot',
    action='store_const',
    const=True,
    help='Plot summed waveforms on screen',
)
plotting_control_group.add_argument(
    '--plot_to_dir',
    help='Save summed waveform plots in directory',
)

# Control events to process
parser.add_argument('--event',
                    type=int,
                    nargs='+',
                    help="Process particular event(s).")
parser.add_argument('--stop_after',
                    type=int,
                    help="Stop after STOP_AFTER events have been processed.")

args = parser.parse_args()

max_cpus = multiprocessing.cpu_count()
if args.cpus == 'all':
    args.cpus = max_cpus
if args.cpus > max_cpus:
    raise ValueError("Can't parallelize over %d cpu's, you only have %d..." % (args.cpus, max_cpus))

##
# Build configuration for paxer
##

# Take config from previous file
if args.redo:
    from pandas import HDFStore
    import json
    file_to_redo = core.data_file_name(args.redo)
    store = HDFStore(file_to_redo)
    if len(store['pax_info']) > 1:
        print("Warning: This file contains data from several processings. Will use config from last!\n")
    override_dict = json.loads(store['pax_info'].iloc[-1]['configuration_json'])
    # Add _redone to output name, unless user has specified an output name
    if not args.output:
        redo_path, ext = os.path.splitext(args.redo)    # Remove .hdf from path
        redo_path += '_redone'
        print("Output path of redone processing not specified: will write to %s\n" % redo_path)
        args.output = redo_path
    store.close()
else:
    override_dict = {'pax': {}}

for argname, configname in (
        ('input',      'input_name'),
        ('output',     'output_name'),
        ('log',        'logging_level'),
        ('stop_after', 'stop_after'),
        ('event',      'events_to_process')):
    value = getattr(args, argname)
    if value is not None:
        override_dict['pax'][configname] = value

# Overrides for plotting
if args.plot_to_dir or args.plot:
    override_dict['pax']['output'] = 'Plotting.PlotEventSummary'
    if args.plot_to_dir:
        override_dict['Plotting.PlotEventSummary'] = {'output_dir': args.plot_to_dir}

##
# Single-core processing
##
if args.cpus == 1:
    pax_instance = core.Processor(config_names=args.config,
                                  config_paths=args.config_path,
                                  config_dict=override_dict)

    try:
        pax_instance.run()
    except (KeyboardInterrupt, SystemExit):
        print("\nShutting down all plugins...")
        pax_instance.shutdown()
        print("Exiting")
        sys.exit()

##
# Parallel processing
##
else:
    if os.name == 'nt':
        raise NotImplementedError("Pax parallelization is not supported under Windows (due to lack of fork).")

    override_dict['pax']['plugin_group_names'] = ['input']
    p = core.Processor(config_names=args.config,
                       config_paths=args.config_path,
                       config_dict=override_dict)

    override_dict['pax']['plugin_group_names'] = ['dsp', 'transform']

    # Establish communication queues
    tasks = multiprocessing.JoinableQueue()
    results = multiprocessing.Queue()

    # Start consumers
    num_consumers = args.cpus
    p.log.info('Creating %d consumers',
               num_consumers)
    consumers = [Consumer(tasks, results,
                          args.config,
                          args.config_path,
                          override_dict) for _ in range(num_consumers)]
    for w in consumers:
        w.start()

    # Enqueue jobs
    num_jobs = 0

    for events in grouper(p.get_events(), 10):
        tasks.put(events)
        num_jobs += 1

    p.shutdown()

    # Add a poison pill for each consumer
    for i in range(num_consumers):
        tasks.put(None)

    # Wait for all of the tasks to finish
    tasks.join()

    override_dict['pax']['plugin_group_names'] = ['output']
    p = core.Processor(config_names=args.config,
                       config_paths=args.config_path,
                       config_dict=override_dict)

    # Start printing results
    while num_jobs:

        for event in results.get():
            p.process_event(event)
        num_jobs -= 1

    p.shutdown()

#!/usr/bin/env python
""" Processor for Analyzing XENON - command line interface """

import argparse
import os
import sys
import copy
import queue
import multiprocessing
from itertools import zip_longest

from pax import core, utils

class ProcessorEvents(multiprocessing.Process):

    def __init__(self, task_queue, result_queue,
                 input_done, process_done, output_done,
                 config_names, config_path, config_dict):
        multiprocessing.Process.__init__(self)

        self.task_queue = task_queue
        self.result_queue = result_queue

        self.input_done = input_done
        self.process_done = process_done
        self.output_done = output_done

        self.config_names = config_names
        self.config_path = config_path
        self.config_dict = config_dict

    def run(self):
        p = core.Processor(config_names=self.config_names,
                           config_paths=self.config_path,
                           config_dict=self.config_dict)

        while True:
            next_events = self.task_queue.get()
            answer = []
            if next_events is None:
                # Poison pill means shutdown
                p.log.info('Exiting')
                p.shutdown()
                self.task_queue.task_done()
                break
            for event in next_events:
                if event is not None:
                    answer.append(p.process_event(event))
            self.task_queue.task_done()
            self.result_queue.put(answer)
        return


class OutputEvents(multiprocessing.Process):

    def __init__(self, task_queue, result_queue,
                 input_done, process_done, output_done,
                 n,
                 config_names, config_path, config_dict):
        multiprocessing.Process.__init__(self)

        self.task_queue = task_queue
        self.result_queue = result_queue

        self.input_done = input_done
        self.process_done = process_done
        self.output_done = output_done

        self.i = 0
        self.n = n

        self.config_names = config_names
        self.config_path = config_path
        self.config_dict = config_dict

    def run(self):
        p = core.Processor(config_names=self.config_names,
                           config_paths=self.config_path,
                           config_dict=self.config_dict)

        while True:
            can_end = self.input_done.is_set() and self.process_done.is_set()
            print('input_done:', self.input_done.is_set(),
                  'process_done:', self.process_done.is_set(),
                  'output_done:', self.output_done.is_set(),
                  'blocks waiting input:', self.task_queue.qsize(),
                  'blocks waiting output:', self.result_queue.qsize())
            print(self.i, self.n, self.i/self.n)
            try:
                next_events = self.result_queue.get(block=True,
                                                    timeout=1) # second

                for event in next_events:
                    if event is not None:
                        self.i += 1
                        p.process_event(event)
                self.result_queue.task_done()

            except queue.Empty as e:
                if can_end:
                    self.output_done.set()
                    p.shutdown()
                    break
                else:
                    continue

        return

parser = argparse.ArgumentParser(description="Process XENON data")
parser.add_argument('--input',  default=None, help="File, database or directory to read events from", nargs='?')
parser.add_argument('--output', default=None, help="File, database or directory to write events to",  nargs='?')

# Parallelization control
parser.add_argument('--cpus', default=1, help="Number of CPUs to use. Default is 1; can be 'all'.",  nargs='?')

# Log level control
parser.add_argument('--log', default=None, help="Set log level, e.g. 'debug'")

# Configuration control
# Pass in a name to use a pre-cooked config from config:
parser.add_argument(
    '--config',
    default='XENON100',
    choices=utils.get_named_configuration_options(),
    nargs='+',
    help="Name(s) of the pax configuration(s) to use."
)
# ... or pass in a path to your own config file:
parser.add_argument(
    '--config_path',
    default=[],
    nargs='+',
    help="Path(s) of the configuration file(s) to use."
)
# ... or load the configuration from a previous processed data file
parser.add_argument(
    '--redo',
    default='',
    help="Path of the HDF5 data file to redo."
)

# Plotting override
plotting_control_group = parser.add_mutually_exclusive_group()
plotting_control_group.add_argument(
    '--plot',
    action='store_const',
    const=True,
    help='Plot summed waveforms on screen',
)
plotting_control_group.add_argument(
    '--plot_to_dir',
    help='Save summed waveform plots in directory',
)

# Control events to process
parser.add_argument('--event',
                    type=int,
                    nargs='+',
                    help="Process particular event(s).")
parser.add_argument('--stop_after',
                    type=int,
                    help="Stop after STOP_AFTER events have been processed.")

args = parser.parse_args()

max_cpus = multiprocessing.cpu_count()
if args.cpus == 'all':
    args.cpus = max_cpus
else:
    args.cpus = int(args.cpus)

##
# Build configuration for paxer
##

# Take config from previous file
if args.redo:
    from pandas import HDFStore
    import json
    file_to_redo = core.data_file_name(args.redo)
    store = HDFStore(file_to_redo)
    if len(store['pax_info']) > 1:
        print("Warning: This file contains data from several processings. Will use config from last!\n")
    override_dict = json.loads(store['pax_info'].iloc[-1]['configuration_json'])
    # Add _redone to output name, unless user has specified an output name
    if not args.output:
        redo_path, ext = os.path.splitext(args.redo)    # Remove .hdf from path
        redo_path += '_redone'
        print("Output path of redone processing not specified: will write to %s\n" % redo_path)
        args.output = redo_path
    store.close()
else:
    override_dict = {'pax': {}}

for argname, configname in (
        ('input',      'input_name'),
        ('output',     'output_name'),
        ('log',        'logging_level'),
        ('stop_after', 'stop_after'),
        ('event',      'events_to_process')):
    value = getattr(args, argname)
    if value is not None:
        override_dict['pax'][configname] = value

# Overrides for plotting
if args.plot_to_dir or args.plot:
    override_dict['pax']['output'] = 'Plotting.PlotEventSummary'
    if args.plot_to_dir:
        override_dict['Plotting.PlotEventSummary'] = {'output_dir': args.plot_to_dir}



##
# Single-core processing
##
if args.cpus == 1:
    pax_instance = core.Processor(config_names=args.config,
                                  config_paths=args.config_path,
                                  config_dict=override_dict)

    try:
        pax_instance.run()
    except (KeyboardInterrupt, SystemExit):
        print("\nShutting down all plugins...")
        pax_instance.shutdown()
        print("Exiting")
        sys.exit()


else:  # Parallel processing
    if os.name == 'nt':
        raise NotImplementedError("Pax parallelization is not supported under "
                                  "Windows (due to lack of fork).")

    override_dict['pax']['plugin_group_names'] = ['input']
    p = core.Processor(config_names=args.config,
                       config_paths=args.config_path,
                       config_dict=copy.deepcopy(override_dict))

    override_dict['pax']['plugin_group_names'] = ['dsp', 'transform']

    # Keep track of processing state that threads can see
    input_done = multiprocessing.Event()
    process_done = multiprocessing.Event()
    output_done = multiprocessing.Event()

    # Establish communication queues
    tasks = multiprocessing.JoinableQueue()
    results = multiprocessing.JoinableQueue()

    # Start consumers
    num_consumers = args.cpus
    p.log.info('Creating %d consumers',
               num_consumers)
    consumers = [ProcessorEvents(tasks, results,
                                 input_done, process_done, output_done,
                                 args.config,
                                 args.config_path,
                                 config_dict=copy.deepcopy(override_dict)) for _ in range(num_consumers)]

    override_dict['pax']['plugin_group_names'] = ['output']
    consumer_output = OutputEvents(tasks, results,
                                   input_done, process_done, output_done,
                                   args.config,
                                   args.config_path,
                                   p.input_plugin.number_of_events,
                                   config_dict=copy.deepcopy(override_dict))

    # Start all worker threads
    for w in consumers + [consumer_output]:
        w.start()

    # Enqueue jobs
    events = []
    n = 10 # chunk in threads (TODO: what is best n?)

    for event in p.get_events():
        events.append(event)
        if len(events) > n:
            tasks.put(events)
            events = []
    else:
        if len(events):
            tasks.put(events)
    p.shutdown()

    input_done.set()

    for i in range(num_consumers):
        tasks.put(None)


    print("Wait for consumers to finish")
    tasks.join()

    process_done.set()

    output_done.wait()
    print("Done")